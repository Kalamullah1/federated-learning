{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4c8e75-ca44-49d0-90c0-cf70af7be685",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e0f67b5bea53442f34ddfb7a4a3c00b",
     "grade": false,
     "grade_id": "cell-800179e4905382c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## CS-E4740 - Federated Learning D (Spring 25)\n",
    "\n",
    "# Assignment 1: ML Basics\n",
    "\n",
    "### R. Gafur, A. Jung\n",
    "\n",
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h2>Deadline: 17.03.2025</h2>\n",
    "</div>\n",
    "\n",
    "<a id='learning_goals'></a>\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Learning Goals\n",
    "<ul>\n",
    "    <li> Access weather data from the Finnish Meteorological Institute (FMI).</li>\n",
    "    <li> Utilize Python libraries (scikit-learn, Keras) to train basic machine learning (ML) models.</li>\n",
    "    <li> Implement regularization techniques via data augmentation.</li>\n",
    "</ul>\n",
    "\n",
    "## Backround Material\n",
    "\n",
    "- Chapter 2 of [FLBook (PDF)](https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FLBook.pdf)\n",
    "- optional: [Linear model implementation in scikitlearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [Decision tree implementation in scikitlearn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html), [Convolutional Neural Networks (CNN) implementation in Keras](https://www.tensorflow.org/tutorials/images/cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526a33d-6927-4022-a093-908604b97e59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd7760e0f9047594733dc3e7d8595903",
     "grade": false,
     "grade_id": "cell-286bdf1b024f8e3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "Before performing any data preprocessing or model training, we begin by loading the dataset and conducting an initial inspection.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load the dataset**: We read the weather data from a CSV file using `pandas`.\n",
    "2. **Check data types**: Displaying column data types helps us understand the structure of the dataset.\n",
    "3. **Inspect the first data point**: We print all feature values of the first record to get an overview of the dataset content.\n",
    "4. **Check dataset dimensions**: The dataset shape is printed to determine the number of rows and columns.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d667f5-cade-4e4c-8478-e308514057c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "366cdd45638974a9fcacec1872bd8b8e",
     "grade": false,
     "grade_id": "cell-01aa4e00280d75e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation station               object\n",
      "Year                               int64\n",
      "Month                              int64\n",
      "Day                                int64\n",
      "Time [Local time]                 object\n",
      "Average temperature [Â°C]         float64\n",
      "Maximum temperature [Â°C]         float64\n",
      "Minimum temperature [Â°C]         float64\n",
      "Average relative humidity [%]      int64\n",
      "Wind speed [m/s]                  object\n",
      "Maximum wind speed [m/s]          object\n",
      "Average wind direction [Â°]        object\n",
      "Maximum gust speed [m/s]          object\n",
      "Precipitation [mm]               float64\n",
      "Average air pressure [hPa]       float64\n",
      "dtype: object\n",
      "\n",
      "******************************\n",
      "\n",
      "First data point:\n",
      "Observation station              Kustavi Isokari\n",
      "Year                                        2023\n",
      "Month                                          4\n",
      "Day                                            1\n",
      "Time [Local time]                          00:00\n",
      "Average temperature [Â°C]                    -0.6\n",
      "Maximum temperature [Â°C]                    -0.4\n",
      "Minimum temperature [Â°C]                    -0.8\n",
      "Average relative humidity [%]                 82\n",
      "Wind speed [m/s]                               6\n",
      "Maximum wind speed [m/s]                     6.4\n",
      "Average wind direction [Â°]                    15\n",
      "Maximum gust speed [m/s]                     7.6\n",
      "Precipitation [mm]                           0.0\n",
      "Average air pressure [hPa]                1007.5\n",
      "Name: 0, dtype: object\n",
      "\n",
      "******************************\n",
      "\n",
      "Dataset Shape: (720, 15)\n"
     ]
    }
   ],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf  # for CNN model (tf.keras)\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
    "# Scikit-learn\n",
    "from sklearn.tree import DecisionTreeRegressor  # for Decision Tree Regressor\n",
    "from sklearn.linear_model import LinearRegression  # for Linear Regressoion\n",
    "from sklearn.metrics import mean_squared_error  # to measure mean squared error (MSE)\n",
    "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv('weather_data.csv')\n",
    "\n",
    "# Check the data types and \n",
    "print(dataset.dtypes)\n",
    "print(\"\\n******************************\\n\")\n",
    "print(\"First data point:\")\n",
    "print(dataset.iloc[0])\n",
    "print(\"\\n******************************\\n\")\n",
    "print(f\"Dataset Shape: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b446c897-f6c0-4163-99a2-38a0163ae88b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "810f813eb3b545fbe3aba5303e525f52",
     "grade": false,
     "grade_id": "cell-1b12822da01a8aac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Turning raw data into datapoints, characterized by features and label. \n",
    "\n",
    "The dataset consists of hourly weather measurements, including:  \n",
    "- **Temperature**: Average, Maximum, Minimum \\([Â°C]\\)  \n",
    "- **Humidity**: Average relative humidity \\([\\%]\\)  \n",
    "- **Wind**: Speed, Maximum speed, Average direction \\([Â°]\\), Maximum gust speed \\([m/s]\\)  \n",
    "- **Precipitation**: Total precipitation \\([mm]\\)  \n",
    "- **Air Pressure**: Average air pressure \\([hPa]\\)  \n",
    "\n",
    "---\n",
    "### Data Points\n",
    "\n",
    "A data point corresponds to a specific hour, e.g., *06-April-2023 from 06:00 - 07:00*, which is recorded as `2023-04-06 06:00:00` (starting time) after preprocessing.\n",
    "\n",
    "- **Features** include all hourly observations from the **previous 5 hours**. Example: For the data point `2023-04-06 06:00:00`, the features correspond to data from *01:00 - 06:00* on the same day.\n",
    "\n",
    "- **Label** is the **temperature recorded 5 hours ahead**. Example: For the data point `2023-04-06 06:00:00`, the label corresponds to the temperature measured during *11:00 - 12:00*.\n",
    "\n",
    "- **Dataset Splits**:\n",
    "  - **Training Set**: Includes data from `2023-04-06 00:00:00` to `2023-04-08 00:00:00`.\n",
    "  - **Validation Set**: Comprises the remaining hours of 2023.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88142fc-2627-4ecd-ba78-13652279e365",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d536d459b8bad0730ebcd2dcb4e784a1",
     "grade": false,
     "grade_id": "cell-ff43b1c863eac8ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (710, 67)\n"
     ]
    }
   ],
   "source": [
    "# Copy the main dataset\n",
    "data = dataset.copy()\n",
    "\n",
    "# Convert specified columns to numeric (handling errors with 'coerce')\n",
    "numeric_columns = [\n",
    "    'Wind speed [m/s]', \n",
    "    'Maximum wind speed [m/s]', \n",
    "    'Average wind direction [Â°]', \n",
    "    'Maximum gust speed [m/s]'\n",
    "]\n",
    "data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill missing values with 0\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Create a 'timestamp' column by combining year, month, day, and local time\n",
    "data['timestamp'] = pd.to_datetime(\n",
    "    data['Year'].astype(str) + '-' +\n",
    "    data['Month'].astype(str) + '-' +\n",
    "    data['Day'].astype(str) + ' ' +\n",
    "    data['Time [Local time]']\n",
    ")\n",
    "data['timestamp'] = data['timestamp'].astype('int64') / 1e9  # Convert to seconds as float\n",
    "\n",
    "# Identify columns for lagged features (excluding non-relevant ones)\n",
    "excluded_columns = ['Observation station', 'timestamp', 'Year', 'Month', 'Day', 'Time [Local time]']\n",
    "columns_to_lag = [col for col in data.columns if col not in excluded_columns]\n",
    "\n",
    "# Create lagged features for the previous 5 hours\n",
    "for lag in range(1, 6):\n",
    "    for col in columns_to_lag:\n",
    "        data[f\"{col} lag_{lag}\"] = data[col].shift(lag)\n",
    "\n",
    "# Define target variable (y) as the average temperature 5 hours ahead\n",
    "data['y'] = data['Average temperature [Â°C]'].shift(-5)\n",
    "\n",
    "# Drop rows with NaN values caused by shifts\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Print dataset shape\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac9df296-7700-4f02-85e0-97b3cf6757ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01d9a249a139156320af2dbfe8471a30",
     "grade": false,
     "grade_id": "cell-84a763857e8d08c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "# Check the data rows number\n",
    "assert data.shape[0] == 710, f\"Unexpected number of rows: {data.shape[0]}.\"\n",
    "\n",
    "print('Sanity check passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4253a9-8927-419b-9f82-49ca77977414",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00c2f6ec41f01d39fb13645dd0ec50e8",
     "grade": false,
     "grade_id": "cell-c39f04529d372b1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ğŸ“Œ TASK 1.1: Split the Data into Training and Validation Sets\n",
    "\n",
    "#### Task Description:\n",
    "1. **Split the dataset** into training (`X_train`, `y_train`) and validation (`X_val`, `y_val`) sets based on the specified time range.\n",
    "2. **Training Set**: Includes data from hours `2023-04-06 00:00:00` to (and including) `2023-04-08 00:00:00`.\n",
    "3. **Validation Set**: Consists of the remaining data.\n",
    "\n",
    "#### Hints:\n",
    "- Use **lagged feature columns** (i.e., columns containing `\"lag\"`) to create feature sets.\n",
    "- Assign **lagged columns** as the feature variables for `X_train` and `X_val`.\n",
    "- Use the **`y` column** as the target variable for `y_train` and `y_val`.\n",
    "    \n",
    "#### Points: 0.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25995df4-2518-4f2a-9a63-99724a8fc247",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd65db493764b31d9800af7086e7f605",
     "grade": false,
     "grade_id": "cell-a47cb8f7b000652c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49, 65), y_train: (49,), X_val: (546, 65), y_val: (546,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_start = pd.to_datetime('2023-04-06 00:00:00').timestamp()  \n",
    "train_end = pd.to_datetime('2023-04-08 00:00:00').timestamp() \n",
    "\n",
    "train_data = data[(data['timestamp'] >= train_start) & (data['timestamp'] <= train_end)]\n",
    "val_data = data[data['timestamp'] > train_end]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = train_data.iloc[:,1:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "X_val = val_data.iloc[:,1:-1]\n",
    "y_val = val_data.iloc[:,-1]\n",
    "\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}, X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "\n",
    "X_train['Time [Local time]'] = X_train['Time [Local time]'].str.split(':').apply(lambda x: float(x[0]) * 60 + float(x[1]))\n",
    "X_val['Time [Local time]'] = X_val['Time [Local time]'].str.split(':').apply(lambda x: float(x[0]) * 60 + float(x[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06281722-2079-44ee-9f0f-d519dcd5c5ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89f84d316777887969c0b78934744a35",
     "grade": true,
     "grade_id": "cell-502cffd8083f6ce4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check the data rows number\n",
    "assert X_train.shape[0] == 49, f\"Unexpected number of rows for X_train: {X_train.shape[0]}.\"\n",
    "assert X_val.shape[0] == 546, f\"Unexpected number of rows for X_val: {X_val.shape[0]}.\"\n",
    "\n",
    "print('Sanity check passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da35b52-35ed-4323-bd54-30cd9f083dd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "001f85136c2210523fbe09aa029a7aac",
     "grade": false,
     "grade_id": "cell-22002a77953c697a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Preparing Data for Model Training: Standardization & Reproducibility\n",
    "\n",
    "After splitting the dataset into training and validation sets, the next step is to **prepare the features** for model training. Many machine learning models, especially those based on gradient-based optimization (e.g., linear regression, neural networks), perform better when features are standardized.\n",
    "\n",
    "#### Why Standardization?\n",
    "Feature standardization ensures that all input variables:\n",
    "- Have a **mean of 0** and **standard deviation of 1**, preventing features with different scales from disproportionately influencing the model.\n",
    "- Enable **faster and more stable convergence** during training.\n",
    "\n",
    "#### Steps:\n",
    "1. **Standardize the Features**:\n",
    "   - The `StandardScaler` from `scikit-learn` is used to transform both the training and validation datasets.\n",
    "   - `fit_transform()` is applied to `X_train` to compute and apply the transformation.\n",
    "   - `transform()` is applied to `X_val` to use the same scaling parameters as `X_train`, ensuring consistency.\n",
    "\n",
    "2. **Ensure Reproducibility**:\n",
    "   - To achieve consistent results across different runs, we set random seeds for both **NumPy** and **TensorFlow**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f0e2c13-358d-44b1-91ae-b17c39b80bef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b567c6bca43ca2b296e9f67a2f14c5d5",
     "grade": false,
     "grade_id": "cell-c4910f0826703290",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                                  int64\n",
      "Month                                 int64\n",
      "Day                                   int64\n",
      "Time [Local time]                   float64\n",
      "Average temperature [Â°C]            float64\n",
      "                                     ...   \n",
      "Maximum wind speed [m/s] lag_5      float64\n",
      "Average wind direction [Â°] lag_5    float64\n",
      "Maximum gust speed [m/s] lag_5      float64\n",
      "Precipitation [mm] lag_5            float64\n",
      "Average air pressure [hPa] lag_5    float64\n",
      "Length: 65, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import the required library for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "# This ensures that the mean is 0 and the standard deviation is 1 for each feature in X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the validation set\n",
    "# We use transform() instead of fit_transform() to ensure the same scaling parameters from X_train are applied\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Set random seed for NumPy to ensure reproducibility in any random operations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set random seed for TensorFlow to ensure reproducibility in model training and initialization\n",
    "tf.random.set_seed(42)\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a77389-dd01-4dc0-9551-da5ee8673df5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0d51256d44d80a1d375ea40c163ad4e",
     "grade": false,
     "grade_id": "cell-6858472768b50e3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ğŸ“Œ TASK 1.2: Linear Regression\n",
    "\n",
    "#### Task Overview:\n",
    "Now that we have **standardized** our features to ensure proper scaling, we can proceed with training our first machine learning model. In this task, you will implement and evaluate a **linear regression model** using the standardized training and validation datasets.\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Train a Linear Regression Model** using `X_train_scaled` as input features.\n",
    "2. **Evaluate Model Performance** by computing the average squared error of the trained model on both the training (`X_train_scaled`,`y_train`) and validation (`X_val_scaled`,`y_val`) sets.\n",
    "\n",
    "#### Points: 1.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef66940-c1aa-4567-8473-04aaa026eb2b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "027a5f2a71cae43dbdd61c43ed656ec5",
     "grade": false,
     "grade_id": "cell-d0a40c1f3f1c707e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************** Linear Regression Diagnosis ***************\n",
      "Training Error (MSE): 0.0000\n",
      "Validation Error (MSE): 626.9899\n"
     ]
    }
   ],
   "source": [
    "#  model training \n",
    "# Model training \n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "X_training_prediction = model.predict(X_train_scaled)  \n",
    "y_validation_prediction = model.predict(X_val_scaled)  \n",
    "# Evaluate the model\n",
    "reg_train_error = mean_squared_error(y_train, X_training_prediction)  \n",
    "reg_val_error = mean_squared_error(y_val, y_validation_prediction)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n*************** Linear Regression Diagnosis ***************\")\n",
    "print(f\"Training Error (MSE): {reg_train_error:.4f}\")\n",
    "print(f\"Validation Error (MSE): {reg_val_error:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92277c5d-9748-4b1c-ba6c-4d9b582f9b13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "037f862271e50afd0b80ca1ad9d49cd8",
     "grade": true,
     "grade_id": "cell-b12fa698e22d62e1",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed! The computed errors are valid numerical values.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Checks: Ensuring Correctness of Model Evaluation\n",
    "\n",
    "# Check if computed errors are numeric values\n",
    "assert isinstance(reg_train_error, float), \"Error: reg_train_error must be a numeric value.\"\n",
    "assert isinstance(reg_val_error, float), \"Error: reg_val_error must be a numeric value.\"\n",
    "\n",
    "print(\"Sanity check passed! The computed errors are valid numerical values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e1001",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ğŸ“Œ TASK 1.3: Convolutional Neural Network (CNN)\n",
    "\n",
    "#### Task Overview:\n",
    "Now that we have trained a **linear regression model**, let's explore a more powerful approach: **a 1D Convolutional Neural Network (CNN)**. CNNs are well-suited for sequential data as they can capture local patterns and temporal dependencies.\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Implement a 1D CNN Model** using `tf.keras.Sequential` with `X_train_scaled` as input.\n",
    "2. **Define the CNN architecture** with the following layers:\n",
    "   - **Input Layer**: Shape `(X_train_scaled.shape[1], 1)`.\n",
    "   - **Multiple Conv1D Layers** with decreasing filter sizes and `ReLU` activation.\n",
    "   - **Flatten Layer** to convert feature maps into a vector.\n",
    "   - **Dense Output Layer** with a single neuron for regression.\n",
    "3. **Compile the Model** with:\n",
    "   - **Optimizer**: Adam\n",
    "   - **Loss Function**: Mean Squared Error (MSE)\n",
    "4. **Train the Model** on the training set.\n",
    "5. **Evaluate Model Performance** on both the training and validation sets (similar to Task 1.2).\n",
    "\n",
    "#### Hints:\n",
    "- Use `tf.keras.layers.Conv1D()` to define the convolutional layers with appropriate `filters`, `kernel_size`, and `activation`.  \n",
    "- Use `tf.keras.layers.Flatten()` to reshape the convolutional output before passing it to the dense layer.  \n",
    "- Use `tf.keras.layers.Dense()` for the final output layer.  \n",
    "- Construct the model using `tf.keras.Sequential()`, and compile it with `model.compile()`.  \n",
    "\n",
    "This CNN will serve as a more expressive alternative to linear regression, allowing us to compare their performances.\n",
    "\n",
    "#### Points: 2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d9272bf-9d85-41dd-8e4f-4bc5344205ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6d6edd6f0c812ac2e4002fa4374e2e",
     "grade": false,
     "grade_id": "cell-247c744a498f1bf1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 875ms/step - loss: 1.8765 - val_loss: 24.5042\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 0.4902 - val_loss: 124.6633\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - loss: 0.6965 - val_loss: 110.3338\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.4379 - val_loss: 56.9381\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 0.1319 - val_loss: 30.0949\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 0.1434 - val_loss: 21.4792\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 0.1710 - val_loss: 20.6815\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 0.1443 - val_loss: 23.4329\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 0.1232 - val_loss: 26.4240\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 0.1246 - val_loss: 26.2086\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - loss: 0.1048 - val_loss: 23.1428\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0734 - val_loss: 20.1810\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 0.0627 - val_loss: 18.9985\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0634 - val_loss: 19.9506\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 0.0592 - val_loss: 22.5293\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 0.0544 - val_loss: 25.3039\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0562 - val_loss: 26.2608\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - loss: 0.0565 - val_loss: 24.5579\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - loss: 0.0481 - val_loss: 21.4426\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step - loss: 0.0387 - val_loss: 18.7089\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - loss: 0.0344 - val_loss: 17.2270\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 0.0317 - val_loss: 16.9207\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - loss: 0.0284 - val_loss: 17.2155\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 0.0266 - val_loss: 17.3884\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0255 - val_loss: 17.0199\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 0.0232 - val_loss: 16.3161\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 0.0207 - val_loss: 15.8122\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 0.0191 - val_loss: 15.8413\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - loss: 0.0175 - val_loss: 16.3075\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - loss: 0.0162 - val_loss: 16.7371\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0159 - val_loss: 16.6189\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0151 - val_loss: 15.9323\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0135 - val_loss: 15.1138\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0123 - val_loss: 14.5854\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 0.0114 - val_loss: 14.4161\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - loss: 0.0107 - val_loss: 14.4047\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 412ms/step - loss: 0.0103 - val_loss: 14.3141\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 0.0098 - val_loss: 14.0992\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - loss: 0.0091 - val_loss: 13.9278\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - loss: 0.0084 - val_loss: 13.9579\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0079 - val_loss: 14.1550\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.0074 - val_loss: 14.3294\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 0.0071 - val_loss: 14.3226\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 0.0067 - val_loss: 14.1795\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - loss: 0.0063 - val_loss: 14.0645\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - loss: 0.0059 - val_loss: 14.0791\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.0056 - val_loss: 14.1835\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - loss: 0.0053 - val_loss: 14.2625\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 0.0050 - val_loss: 14.2871\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - loss: 0.0047 - val_loss: 14.3300\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0035\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.2943\n",
      "\n",
      "*************** Convolutional Neural Network (CNN) Diagnosis ***************\n",
      "Training error: 0.003981698304414749\n",
      "Validation error: 14.329957962036133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#shaping the dataset to fit conv_nn format\n",
    "X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_val_cnn = X_val_scaled.reshape((X_val_scaled.shape[0], X_val_scaled.shape[1], 1))\n",
    "\n",
    "#defining our model parameters\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "#model training\n",
    "cnn_history =  cnn_model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_data=(X_val_cnn, y_val))\n",
    "\n",
    "#perdormance evaluation\n",
    "cnn_train_error = cnn_model.evaluate(X_train_cnn, y_train)\n",
    "cnn_val_error = cnn_model.evaluate(X_val_cnn, y_val)\n",
    "\n",
    "print(\"\\n*************** Convolutional Neural Network (CNN) Diagnosis ***************\")\n",
    "print(\"Training error:\", cnn_train_error)\n",
    "print(\"Validation error:\", cnn_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28070c83-619e-43d9-90e7-2b8ed95d1494",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3304d2f8f488d583f6e9cd5191af2ab9",
     "grade": true,
     "grade_id": "cell-00123b0fc7edc4c6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(cnn_train_error, float), \"cnn_train_error must be a numeric value.\"\n",
    "assert isinstance(cnn_val_error, float), \"cnn_val_error must be a numeric value.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d015bbc-35fb-458d-8563-99df1095257d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "385fc9b5cedc0874b7e9979c1ddee50c",
     "grade": false,
     "grade_id": "cell-c69536fcc8dadf2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='varying_features'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ğŸ“Œ TASK 1.4: Decision Tree Regressor\n",
    "\n",
    "#### Task Overview:\n",
    "Building on our previous models, we will now implement a **Decision Tree Regressor**, a non-parametric model that can capture nonlinear relationships in the data.\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Train a Decision Tree Regressor** using `X_train_scaled` as input features with `max_depth=3`.\n",
    "2. **Compute Mean Squared Error (MSE)** for training (`X_train_scaled`,`y_train`) and validation (`X_val_scaled`,`y_val`) sets.\n",
    "\n",
    "#### Points: 1.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb7552de-4da0-480b-9843-5a3f004805b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02b1a503eea2faabae63e267d5556acf",
     "grade": false,
     "grade_id": "cell-d9093756d3e2a7b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# tree_train_error =\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# tree_val_error = \u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m*************** Decision Tree Regressor Diagnosis ***************\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining error:\u001b[39m\u001b[33m\"\u001b[39m, tree_train_error)\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# tree_train_error =\n",
    "# tree_val_error = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"\\n*************** Decision Tree Regressor Diagnosis ***************\")\n",
    "print(\"Training error:\", tree_train_error)\n",
    "print(\"Validation error:\", tree_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8297aa6-e501-4e00-b582-c21e57cf39c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3ff45fdb75d219e4399e9f43ca81ed1",
     "grade": true,
     "grade_id": "cell-ebda409a3c406b3f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(tree_train_error, float), \"tree_train_error must be a numeric value.\"\n",
    "assert isinstance(tree_val_error, float), \"tree_val_error must be a numeric value.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b104dcd-4e18-4775-9ae7-6e6605795b39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4ff13728c8b455012602d09d8614741",
     "grade": false,
     "grade_id": "cell-a33c51b6d2c08951",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Regularization \n",
    "\n",
    "So far, we have trained **three basic ML models**, a linear model, a CNN and a decision tree. While these models can effectively learn patterns from the training data, they are **prone to overfitting**, especially when the dataset is small. To improve generalization, we can use **regularization techniques** such as adding the **ridge penalty**.\n",
    "\n",
    "### Implementing the Ridge Penalty with Data Augmentation  \n",
    "\n",
    "To implement **ridge regularization**, we will:\n",
    "1. **Generate an augmented training set** using independent and identically distributed (i.i.d.) **Gaussian random vectors**.\n",
    "2. **Set the labels** for this augmented data to **0**.\n",
    "3. **Modify our existing models** (Linear Regression, CNN, and Decision Tree) to incorporate this augmented dataset.\n",
    "\n",
    "This approach introduces **a penalty term**, encouraging models to keep their predictions closer to zero for these artificial data points, thus improving stability and reducing overfitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6a741-6d78-4142-91e6-a0c034773f22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75c14f4ffd334893753633895fd64bc5",
     "grade": false,
     "grade_id": "cell-1e50876935e180af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_augmented_dataset(X_train, y_train, augmentation_ratio=0.5, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generates an augmented training set by adding synthetic data points drawn from a Gaussian distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (numpy array or DataFrame): Original training feature set.\n",
    "    - y_train (numpy array or Series): Original training labels.\n",
    "    - augmentation_ratio (float): Ratio of synthetic samples to real training samples (default: 0.5).\n",
    "    - random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_aug (numpy array): Augmented feature set.\n",
    "    - y_train_aug (numpy array): Augmented labels (including original and synthetic samples).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seed for full reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)  # Ensure reproducibility in TensorFlow if used later\n",
    "\n",
    "    # Determine the number of augmented samples\n",
    "    num_augmented_samples = int(augmentation_ratio * X_train.shape[0])\n",
    "\n",
    "    # Compute mean and standard deviation of each feature in X_train\n",
    "    feature_means = np.mean(X_train, axis=0)\n",
    "    feature_stds = np.std(X_train, axis=0) + 1e-8  # Small epsilon to avoid zero variance issues\n",
    "\n",
    "    # Generate synthetic feature vectors from a Gaussian distribution\n",
    "    X_augmented = np.random.normal(loc=feature_means, scale=feature_stds, size=(num_augmented_samples, X_train.shape[1]))\n",
    "\n",
    "    # Set the labels for the augmented data points to 0\n",
    "    y_augmented = np.zeros(num_augmented_samples)\n",
    "\n",
    "    # Combine the original and augmented datasets\n",
    "    X_train_aug = np.vstack((X_train, X_augmented))\n",
    "    y_train_aug = np.hstack((y_train, y_augmented))\n",
    "\n",
    "    return X_train_aug, y_train_aug\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train_scaled and y_train are already defined\n",
    "X_train_aug, y_train_aug = generate_augmented_dataset(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Original training set shape:\", X_train_scaled.shape)\n",
    "print(\"Augmented training set shape:\", X_train_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc850685-fc20-46ba-9130-d0c2e881c634",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ecbc646bdc0f27dea5738da50effc5e",
     "grade": false,
     "grade_id": "cell-86a58c1344835504",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='regularized_models'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### ğŸ“Œ TASK 1.5: Regularized Model Training with Augmented Data\n",
    "\n",
    "In this task, you will **regularize the training** of all three models from previous tasksâ€”**Linear Regression, CNN, and Decision Tree Regressor**â€”by replacing the original training set with the **augmented training set** generated above. The augmented dataset contains **synthetic feature vectors drawn from a Gaussian distribution**, with their labels set to **0**. For parametric models, using this augmented dataset to train ML a model is equivalent to adding a ridge penalty term (see Sec. 6.6. of [MLBook (PDF)](https://mlbook.cs.aalto.fi)).\n",
    "\n",
    "#### Task Instructions:\n",
    "1. **Train the following models using the augmented training set (`X_train_aug`, `y_train_aug`)**:\n",
    "   - **Linear Regression**\n",
    "   - **Convolutional Neural Network (CNN)**\n",
    "   - **Decision Tree Regressor**\n",
    "2. **Compute Mean Squared Error (MSE)** of these regularized models on the original training (`X_train_scaled`,`y_train`) and validation sets (`X_val_scaled`,`y_val`).\n",
    "\n",
    "#### Points: 1.5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa57f9a-88b8-49eb-bc2f-12b7b13d041b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2792690b475d42157fea35acc6de50f",
     "grade": false,
     "grade_id": "cell-e04b32ef36edd4ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Linear Regression using augmented training set\n",
    "# custom_reg =\n",
    "\n",
    "# Convolutional Neural Network (CNN) using augmented training set\n",
    "# custom_cnn =\n",
    "# custom_cnn.compile(...)\n",
    "# custom_cnn =  \n",
    "\n",
    "# Decision Tree Regressor using augmented training set\n",
    "# custom_tree =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "custom_reg_train_error = mean_squared_error(y_train, custom_reg.predict(X_train_scaled)) \n",
    "custom_reg_val_error = mean_squared_error(y_val, custom_reg.predict(X_val_scaled)) \n",
    "\n",
    "print(\"\\n*************** Diagnosis of Regularized Linear Model ***************\")\n",
    "print(\"Training error:\", custom_reg_train_error)\n",
    "print(\"Validation error:\", custom_reg_val_error)\n",
    "\n",
    "custom_cnn_train_error = custom_cnn.evaluate(X_train_scaled[..., np.newaxis], y_train, verbose=0)\n",
    "custom_cnn_val_error = custom_cnn.evaluate(X_val_scaled[..., np.newaxis], y_val, verbose=0)\n",
    "\n",
    "print(\"\\n*************** Diagnosis of Regularized CNN ***************\")\n",
    "print(\"Training error:\", custom_cnn_train_error)\n",
    "print(\"Validation error:\", custom_cnn_val_error)\n",
    "\n",
    "custom_tree_train_error = mean_squared_error(y_train, custom_tree.predict(X_train_scaled))\n",
    "custom_tree_val_error = mean_squared_error(y_val, custom_tree.predict(X_val_scaled))\n",
    "\n",
    "print(\"\\n*************** Diagnosis of Regularized Decision Tree ***************\")\n",
    "print(\"Training error:\", custom_tree_train_error)\n",
    "print(\"Validation error:\", custom_tree_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785a988-b4cc-40ff-9b2b-66bf46c0a696",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72e87c65743bfa629d2a3f5241c227b0",
     "grade": true,
     "grade_id": "cell-7679a24953932c7f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(reg_train_error, float), \"custom_reg_train_error must be a numeric value.\"\n",
    "assert isinstance(reg_val_error, float), \"custom_reg_val_error must be a numeric value.\"\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(cnn_train_error, float), \"cnn_train_error must be a numeric value.\"\n",
    "assert isinstance(cnn_val_error, float), \"cnn_val_error must be a numeric value.\"\n",
    "\n",
    "# Check if the variables store numeric values\n",
    "assert isinstance(tree_train_error, float), \"tree_train_error must be a numeric value.\"\n",
    "assert isinstance(tree_val_error, float), \"tree_val_error must be a numeric value.\"\n",
    "\n",
    "print('Sanity check passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55c6d4-511e-4bf0-a87b-9fe4843d9170",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b7a3b95f0cebf98b4ce32bad6160af0",
     "grade": false,
     "grade_id": "cell-9ac5ab50b779305c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51371567-0b10-436b-8908-408c27a60fae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b812fd35564dd0c99766833d34085bda",
     "grade": false,
     "grade_id": "cell-5d9a1624d9fde6b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_train_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[141]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m models = [\u001b[33m\"\u001b[39m\u001b[33mLinear Model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCNN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDecision Tree\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Training errors\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m train_errors = [reg_train_error, \u001b[43mcnn_train_error\u001b[49m, tree_train_error]\n\u001b[32m     10\u001b[39m custom_train_errors = [custom_reg_train_error, custom_cnn_train_error, custom_tree_train_error]\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Validation errors\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'cnn_train_error' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define model names\n",
    "models = [\"Linear Model\", \"CNN\", \"Decision Tree\"]\n",
    "\n",
    "# Training errors\n",
    "train_errors = [reg_train_error, cnn_train_error, tree_train_error]\n",
    "custom_train_errors = [custom_reg_train_error, custom_cnn_train_error, custom_tree_train_error]\n",
    "\n",
    "# Validation errors\n",
    "val_errors = [reg_val_error, cnn_val_error, tree_val_error]\n",
    "custom_val_errors = [custom_reg_val_error, custom_cnn_val_error, custom_tree_val_error]\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "error_data = pd.DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Training Error (Original)\": train_errors,\n",
    "    \"Training Error (Regularized)\": custom_train_errors,\n",
    "    \"Validation Error (Original)\": val_errors,\n",
    "    \"Validation Error (Regularized)\": custom_val_errors\n",
    "})\n",
    "\n",
    "# Print the table\n",
    "print(error_data.to_string(index=False))\n",
    "\n",
    "# Define x-axis positions for grouped bar charts\n",
    "x = np.arange(len(models))  # Model positions\n",
    "width = 0.3  # Bar width for better visualization\n",
    "\n",
    "# Plot Training Errors\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x - width/2, train_errors, width, label='Original Model', color='blue', alpha=0.7)\n",
    "plt.bar(x + width/2, custom_train_errors, width, label='Regularized Model', color='red', alpha=0.7)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Training Error\")\n",
    "plt.title(\"Training Errors (Regularized vs. Non-Regularized)\")\n",
    "plt.xticks(ticks=x, labels=models)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Validation Errors\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x - width/2, val_errors, width, label='Original Model', color='blue', alpha=0.7)\n",
    "plt.bar(x + width/2, custom_val_errors, width, label='Regularized Model', color='red', alpha=0.7)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.title(\"Validation Errors (Regularized vs. Non-Regularized)\")\n",
    "plt.xticks(ticks=x, labels=models)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
